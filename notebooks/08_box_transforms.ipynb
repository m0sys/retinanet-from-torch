{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Box Parameterization Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec(\"model\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization for BBox Regression.\n",
    "paper: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value for clamping large dw and dh predictions. The heuristic is that we clamp\n",
    "# such that dw and dh are no larger than what would transform a 16px box into a\n",
    "# 1000px box (based on a small anchor, 16px, and a typical image size, 1000px).\n",
    "_DEFAULT_SCALE_CLAMP = math.log(1000.0 / 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box2BoxTransform:\n",
    "    \"\"\"\n",
    "    The box-to-box transform defined in R-CNN. The transformation is parameterized\n",
    "    by 4 deltas: (dx, dy, dw, dh). The transformation scales the box's width and height\n",
    "    by exp(dw), exp(dh) and shifts a box's center by the offset (dx * width, dy * height).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, weights: Tuple[float, float, float, float], scale_clamp: float = _DEFAULT_SCALE_CLAMP\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights: Scaling factors that are applied to the (dx, dy, dw, dh) deltas.\n",
    "                In \"Fast R-CNN\" these were originally set such that the deltas have unit var.\n",
    "                Here they are treated as hyperparameters of the system.\n",
    "            scale_clamp: When predicting deltas the predicted box scaling factors (dw, dh) are\n",
    "                clamped such that they are <= scale_clamp.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.scale_clamp = scale_clamp\n",
    "        \n",
    "    def get_deltas(self, src_boxes, target_boxes):\n",
    "        \"\"\"\n",
    "        Get deltas as defined in R-CNN paper between `src_boxes` and `target_boxes`.\n",
    "        \n",
    "        Calculate box regression transformation deltas (dx, dy, dw, dh) to be used\n",
    "        to transform the `src_boxes` into the `target_boxes`.\n",
    "        \n",
    "        That is, the relation ``target_boxes == self.apply_deltas(deltas, src_boxes)``\n",
    "        is true (unless any delta is too large and is clamped).\n",
    "\n",
    "        Args:\n",
    "            src_boxes (Tensor): source boxes, e.g., object proposals in boundary coords.\n",
    "            target_boxes (Tensor): target of the transformation, e.g., ground-truth\n",
    "                boxes in boundary coords.\n",
    "        \"\"\"\n",
    "        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)\n",
    "        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)\n",
    "        \n",
    "        src_widths = src_boxes[:, 2] - src_boxes[:, 0]\n",
    "        src_heights = src_boxes[: 3] - src_boxes[:, 1]\n",
    "        src_ctr_x = src_boxes[:, 0] + 0.5 * src_widths\n",
    "        src_ctr_y = src_boxes[: 1] + 0.5 * src_heights\n",
    "        \n",
    "        target_widths = target_boxes[:, 2] - target_boxes[:, 0]\n",
    "        target_heights = target_boxes[: 3] - target_boxes[:, 1]\n",
    "        target_ctr_x = target_boxes[:, 0] + 0.5 * target_widths\n",
    "        target_ctr_y = target_boxes[: 1] + 0.5 * target_heights\n",
    "        \n",
    "        wx, wy, ww, wh = self.weights\n",
    "        dx = wx * (target_ctr_x - src_ctr_x) / src_widths\n",
    "        dy = wy * (target_ctr_y - src_ctr_y) / src_heights\n",
    "        dw = ww * torch.log(target_widths / src_widths)\n",
    "        dh = wh * torch.log(target_heights / src_heights)\n",
    "        \n",
    "        deltas = torch.stack((dx, dy, dw, dh), dim=1)\n",
    "        \n",
    "        assert (src_widths > 0).all().item(), \"Input boxes to Box2BoxTransform are not valid!\" \n",
    "        return deltas\n",
    "    \n",
    "    def apply_deltas(self, deltas, boxes):\n",
    "        \"\"\"\n",
    "        Apply transformation `deltas` to `boxes`.\n",
    "        \n",
    "        Args:\n",
    "            deltas (Tensor): transformation deltas of shape (N, k*4), where k >=1.\n",
    "                deltas[i] represents k potentially different class-specific box \n",
    "                transformations for the single box boxes[i].\n",
    "            boxes (Tensor): boxes to transform, of shape (N, 4)\n",
    "        \"\"\"\n",
    "        \n",
    "        deltas = deltas.float()  # ensure fp32 for decoding precision\n",
    "        boxes = boxes.to(deltas.dtype)\n",
    "        \n",
    "        widths = boxes[:, 2] - boxes[:, 0]\n",
    "        heights = boxes[:, 3] - boxes[:, 1]\n",
    "        ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "        ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "        \n",
    "        wx, wy, ww, wh = self.weights\n",
    "        dx = deltas[:, 0::4] / wx\n",
    "        dy = deltas[:, 1::4] / wy\n",
    "        dw = deltasl[:, 2::4] / ww        \n",
    "        dh = deltas[:, 3::4] / wh\n",
    "        \n",
    "        # Prevent sending too large values into torch.exp()\n",
    "        dw = torch.clamp(dw, max=self.scale_clamp) \n",
    "        dh = torch.clamp(dh, max=self.scale_clamp) \n",
    "        \n",
    "        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n",
    "        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n",
    "        pred_w = torch.exp(dw) * widths[:, None]\n",
    "        pred_h = torch.exp(dh) * heights[:, None]\n",
    "        \n",
    "        pred_boxes = torch.zeros_like(deltas)\n",
    "        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_trfm = Box2BoxTransform([1., 1., 1., 1.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
