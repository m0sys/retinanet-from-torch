{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of RetinaNet Anchor Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec(\"model\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Tuple\n",
    "import math\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retina Anchors.\n",
    "\n",
    "paper: [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "Section 4. RetinaNet Detector -> Anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferList(nn.Module):\n",
    "    def __init__(self, buffers):\n",
    "        super().__init__()\n",
    "        for i, buffer in enumerate(buffers):\n",
    "            self.register_buffer(str(i), buffer)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._buffers)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self._buffers.values())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self._buffers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _broadcast_params(params: Union[List[float], Tuple[float]], num_features: int):\n",
    "    if not isinstance(params[0], (list, tuple)):\n",
    "        return [params] * num_features\n",
    "    if len(params) == 1:\n",
    "        return list(params) * num_features\n",
    "    \n",
    "    assert len(params) == num_features\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBoxGenerator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sizes: List[float],\n",
    "                 aspect_ratios: List[float],\n",
    "                 strides: List[int],\n",
    "                 scales: Optional[List[float]] = [1.0],\n",
    "                 offset: Optional[float]=0.5\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Compute anchors in the standard way described in\n",
    "        \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\"\n",
    "        paper.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.strides = strides\n",
    "        self.num_features = len(self.strides)\n",
    "        sizes = _broadcast_params([[size * scale for scale in scales] for size in sizes], self.num_features)\n",
    "        aspect_ratios = _broadcast_params(aspect_ratios, self.num_features)\n",
    "        self.cell_anchors = self._calculate_anchors(sizes , aspect_ratios)\n",
    "        \n",
    "        \n",
    "        print(f\"sizes: {sizes}\")\n",
    "        print(f\"aspect ratios: {aspect_ratios}\")\n",
    "        self.offset = offset\n",
    "        assert 0.0 <= self.offset < 1.0\n",
    "        \n",
    "    def _calculate_anchors(self, sizes, aspect_ratios):\n",
    "        cell_anchors = [\n",
    "            self.generate_anchor_boxes(s, a).float() for s, a in zip(sizes, aspect_ratios)\n",
    "        ]\n",
    "        return cell_anchors\n",
    "        return BufferList(cell_anchors)\n",
    "        \n",
    "    def generate_anchor_boxes(self, sizes=(32, 128, 256, 512), aspect_ratios=(0.5, 1, 2)):\n",
    "        \"\"\"\n",
    "        Generate a tensor storing canonical anchor boxes of different \n",
    "        sizes and aspect ratios centered at (0, 0). \n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (len(sizes) * len(aspect_ratios), 4) storing anchor boxes in\n",
    "            bounding-box (X_min, Y_min, X_max, Y_max) coords.\n",
    "        \"\"\"\n",
    "        \n",
    "        anchors = []\n",
    "        for size in sizes:\n",
    "            area = size ** 2.0\n",
    "            for aspect_ratio in aspect_ratios:\n",
    "                w = math.sqrt(area / aspect_ratio)\n",
    "                h = aspect_ratio * w\n",
    "                x0, y0, x1, y1 = -w / 2.0, -h / 2.0, w / 2.0, h / 2.0 # centered @ (0, 0)\n",
    "                anchors.append([x0, y0, x1, y1])\n",
    "                \n",
    "        return torch.tensor(anchors)\n",
    "    \n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: list of backbone feature maps on which to generate anchors.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Note that the generated anchors depend on the feature maps and not the \n",
    "        # gt images themselves. We are generating anchors w.r.t to feature maps.\n",
    "        \n",
    "        grid_sizes = [feature_map.shape[-2:] for feature_map in features]\n",
    "        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)\n",
    "        \n",
    "        return anchors_over_all_feature_maps\n",
    "        \n",
    "    def _grid_anchors(self, grid_sizes: List[List[int]]):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list[Tensor]: #feature map tensors of shape (locs x cell_anchors) * 4\n",
    "        \"\"\"\n",
    "        anchors = []\n",
    "        buffers = [x[1] for x in self.cell_anchors.named_buffers()]\n",
    "        \n",
    "        for size, stride, base_anchors in zip(grid_sizes, self.strides, buffers):\n",
    "            shift_x, shift_y = self._create_grid_offsets(size, stride, self.offset, base_anchors.device)\n",
    "            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)\n",
    "            \n",
    "            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))\n",
    "            \n",
    "        return anchors\n",
    "    def _create_grid_offsets(size: List[int], stride: int, offset: float, device: torch.device):\n",
    "        grid_height, grid_width = size\n",
    "        shifts_x = torch.arange(\n",
    "            offset * stride, grid_width * stride, step=stride, dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        shifts_y = torch.arange(\n",
    "            offset * stride, grid_height * stride, step=stride, dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "        shift_x, = shift_x.reshape(-1)\n",
    "        shift_y = shift_y.reshape(-1)\n",
    "        \n",
    "        return shift_x, shift_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: [[32.0, 40.31747359663594, 50.79683366298238], [64.0, 80.63494719327188, 101.59366732596476], [128.0, 161.26989438654377, 203.18733465192952], [256.0, 322.53978877308754, 406.37466930385904], [512.0, 645.0795775461751, 812.7493386077181]]\n",
      "aspect ratios: [[0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0], [0.5, 1.0, 2.0]]\n"
     ]
    }
   ],
   "source": [
    "anchors = AnchorBoxGenerator(\n",
    "    sizes=[32., 64., 128., 256., 512.],\n",
    "    aspect_ratios=[0.5, 1., 2.],\n",
    "    scales=[1., 2 ** (1 / 3), 2 ** (2 / 3)],\n",
    "    strides=[2, 2, 2, 2, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-22.6274, -11.3137,  22.6274,  11.3137],\n",
       "         [-16.0000, -16.0000,  16.0000,  16.0000],\n",
       "         [-11.3137, -22.6274,  11.3137,  22.6274],\n",
       "         [-28.5088, -14.2544,  28.5088,  14.2544],\n",
       "         [-20.1587, -20.1587,  20.1587,  20.1587],\n",
       "         [-14.2544, -28.5088,  14.2544,  28.5088],\n",
       "         [-35.9188, -17.9594,  35.9188,  17.9594],\n",
       "         [-25.3984, -25.3984,  25.3984,  25.3984],\n",
       "         [-17.9594, -35.9188,  17.9594,  35.9188]]),\n",
       " tensor([[-45.2548, -22.6274,  45.2548,  22.6274],\n",
       "         [-32.0000, -32.0000,  32.0000,  32.0000],\n",
       "         [-22.6274, -45.2548,  22.6274,  45.2548],\n",
       "         [-57.0175, -28.5088,  57.0175,  28.5088],\n",
       "         [-40.3175, -40.3175,  40.3175,  40.3175],\n",
       "         [-28.5088, -57.0175,  28.5088,  57.0175],\n",
       "         [-71.8376, -35.9188,  71.8376,  35.9188],\n",
       "         [-50.7968, -50.7968,  50.7968,  50.7968],\n",
       "         [-35.9188, -71.8376,  35.9188,  71.8376]]),\n",
       " tensor([[ -90.5097,  -45.2548,   90.5097,   45.2548],\n",
       "         [ -64.0000,  -64.0000,   64.0000,   64.0000],\n",
       "         [ -45.2548,  -90.5097,   45.2548,   90.5097],\n",
       "         [-114.0350,  -57.0175,  114.0350,   57.0175],\n",
       "         [ -80.6349,  -80.6349,   80.6349,   80.6349],\n",
       "         [ -57.0175, -114.0350,   57.0175,  114.0350],\n",
       "         [-143.6751,  -71.8376,  143.6751,   71.8376],\n",
       "         [-101.5937, -101.5937,  101.5937,  101.5937],\n",
       "         [ -71.8376, -143.6751,   71.8376,  143.6751]]),\n",
       " tensor([[-181.0193,  -90.5097,  181.0193,   90.5097],\n",
       "         [-128.0000, -128.0000,  128.0000,  128.0000],\n",
       "         [ -90.5097, -181.0193,   90.5097,  181.0193],\n",
       "         [-228.0701, -114.0350,  228.0701,  114.0350],\n",
       "         [-161.2699, -161.2699,  161.2699,  161.2699],\n",
       "         [-114.0350, -228.0701,  114.0350,  228.0701],\n",
       "         [-287.3503, -143.6751,  287.3503,  143.6751],\n",
       "         [-203.1873, -203.1873,  203.1873,  203.1873],\n",
       "         [-143.6751, -287.3503,  143.6751,  287.3503]]),\n",
       " tensor([[-362.0387, -181.0193,  362.0387,  181.0193],\n",
       "         [-256.0000, -256.0000,  256.0000,  256.0000],\n",
       "         [-181.0193, -362.0387,  181.0193,  362.0387],\n",
       "         [-456.1401, -228.0701,  456.1401,  228.0701],\n",
       "         [-322.5398, -322.5398,  322.5398,  322.5398],\n",
       "         [-228.0701, -456.1401,  228.0701,  456.1401],\n",
       "         [-574.7006, -287.3503,  574.7006,  287.3503],\n",
       "         [-406.3747, -406.3747,  406.3747,  406.3747],\n",
       "         [-287.3503, -574.7006,  287.3503,  574.7006]])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.cell_anchors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
