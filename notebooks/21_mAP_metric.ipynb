{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and Implementing the mAP Metric for Object Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec(\"model\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Mean Average Precision (mAP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Precision and Recall\n",
    "\n",
    "First we need to understand what we mean by `Precision` and `Recall`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$precision = \\frac{TP}{TP + FP}\\text{, } recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Precision?\n",
    "\n",
    "Precision measures the amount of \"Truthiness\" when the model predicts that a label is true. In other words, it represents the `ratio of True Positives w.r.t all predictions that are predicted as true (True Positives + False Positives).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Recall?\n",
    "\n",
    "Recall on the other hand measures the ability to detect Truth. In other words, it represents the `ratio of True Positives w.r.t all instances of there being Truth (True Positives + False Negatives.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Average Precision\n",
    "\n",
    "Now that we know what Precision and Recall mean we can define `Average Precision` easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Average Precision?\n",
    "\n",
    "Average Precision is the simply taking the average of all maximum possible precisions for any given amount of Recall within out interest. For example, if we have two different Precisions with one being 90% and the other being 96% given a Recall value of 10% we select the maximal Precision which in this case is 96%. Now, lets say that we are interested in finding the Average Precision for Recall values of 10% increaments. Then we would basically have to sum up all maximal Precisions for each 10% increament and divide the amount by 10 to get the Average Precision. In other words, Average Precision is `the sum of maximal Precisions over the n Recall values of interest divided by n.`\n",
    "\n",
    "$$Average Precision = \\frac{\\sum_{i = n}Precision_{max}(Recall_i)}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Mean Average Precision \n",
    "\n",
    "Since we now understand understand Avergage Precision we are ready to delve into `Mean Average Precision.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Mean Average Precision (mAP)?\n",
    "\n",
    "The Mean Average Precision is used most popularily in Object Detection tasks. So, we will take Object Detection as the running example.\n",
    "\n",
    "Each prediction in a multiclass Object Detection problem can have multiple classes associated with it. So to have a metric to measure how well the model is doing we can simply take the literal \"mean\" of all Average Precisions over all classes. However, this is only a metric so far that measures how good the classification part of the problem is doing. What about localization? Well we can simply use the `IOU (Intersection Over Union)` to threshold whether a prediction that has been labeled correctly is considered truly a True Positive. In other words, we use the IOU as a filtering method for how good the True Positive lables are. By doing so we are effectively able to also measure the \"goodness\" of the predicted bounding boxes. For example, if our threshold is `IOU = 50%` then we can calculate $mAP_{50}$ by taking the mean of the Average Precision over all classes at the 50% mark. \n",
    "\n",
    "However, there is one more level that to this in the case of the COCO dataset where we take the mean of the mAPs (something like a mean Mean Average Precision) over multiple different IOU thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou_single(pred_box, gt_box):\n",
    "    \"\"\"Calculate IoU of single predicted and ground truth box\n",
    "    Args:\n",
    "        pred_box (list of floats): location of predicted object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "        gt_box (list of floats): location of ground truth object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "            \n",
    "    Returns:\n",
    "        float: value of the IoU for the two boxes.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: if the box is obviously malformed\n",
    "    \"\"\"\n",
    "    x1_t, y1_t, x2_t, y2_t = gt_box\n",
    "    x1_p, y1_p, x2_p, y2_p = pred_box\n",
    "    \n",
    "    if (x1_p > x2_p) or (y1_p > y2_p):\n",
    "        raise AssertionError(\n",
    "            \"Prediction box is malformed? pred box: {}\".format(pred_box))\n",
    "    \n",
    "    if (x1_t > x2_t) or (y1_t > y2_t):\n",
    "        raise AssertionError(\n",
    "            \"Ground Truth box is malformed? true box: {}\".format(gt_box))\n",
    "        \n",
    "    if (x2_t < x1_p or x2_p < x1_t or y2_t < y1_p or y2_p < y1_t):\n",
    "        return 0.0\n",
    "    \n",
    "    far_x  = np.min([x2_t, x2_p])\n",
    "    near_x = np.max([x1_t, x1_p])\n",
    "    far_y = np.min([y2_t, y2_p])\n",
    "    near_y = np.max([y1_t, y1_p])\n",
    "    \n",
    "    inter_area = (far_x - near_x + 1) * (far_y - near_y + 1)\n",
    "    true_box_area = (x2_t - x1_t + 1) * (y2_t - y1_t + 1)\n",
    "    pred_box_area = (x2_p - x1_p + 1) * (y2_p - y1_p + 1)\n",
    "    iou = inter_area / (true_box_area + pred_box_area - inter_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thresh):\n",
    "    \"\"\"\n",
    "    Get results for a single batch.\n",
    "    \n",
    "    Calculate the number of `true_pos`, `false_pos`, `false_neg`.\n",
    "    \n",
    "    Args:\n",
    "    gt_boxes (list of list of floats): list of locations of ground truth\n",
    "        objects as [xmin, ymin, xmax, ymax]\n",
    "    pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "        and 'scores'\n",
    "    iou_thresh (float): value of IoU to consider as threshold for a\n",
    "        true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    \n",
    "    if len(all_pred_indices) == 0:\n",
    "        return {'true_pos': 0, 'false_pos': 0, 'false_neg': len(gt_boxes)}\n",
    "    \n",
    "    if len(all_gt_indices) == 0:\n",
    "        return {'true_pos': 0, 'false_pos': len(pred_boxes), 'false_neg': 0}\n",
    "    \n",
    "    gt_idx_thresh, pred_idx_thresh, ious = [], [], []\n",
    "    \n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou_single(pred_box, gt_box)\n",
    "            if iou > iou_thresh:\n",
    "                gt_idx_thresh.append(igb)\n",
    "                pred_idx_thresh.append(ipb)\n",
    "                ious.append(iou)\n",
    "    \n",
    "    args_desc = np.argsort(ious)[::-1]\n",
    "    \n",
    "    if len(args_desc) == 0:\n",
    "        # No Matches.\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = len(gt_boxes)\n",
    "        \n",
    "    else:\n",
    "        gt_match_idx = []\n",
    "        pred_match_idx = []\n",
    "        \n",
    "        for idx in args_desc:\n",
    "            gt_idx = gt_idx_thresh[idx]\n",
    "            pred_idx = pred_idx_thresh[idx]\n",
    "            \n",
    "            # If the boxes are unmatched, add them to matches.\n",
    "            if (gt_idx not in gt_match_idx) and (pred_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pred_idx)\n",
    "        tp = len(gt_match_idx)\n",
    "        fp = len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "        \n",
    "    return {\"true_pos\": tp, \"false_pos\": fp, \"false_neg\": fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(img_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for _, res in img_results.items():\n",
    "        tp += res[\"true_pos\"]\n",
    "        fp += res[\"false_pos\"]\n",
    "        fn += res[\"false_neg\"]\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    \n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "        \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores_map(pred_boxes):\n",
    "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "    Args:\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "    Returns:\n",
    "        dict: keys are model_scores and values are image ids (usually filenames)\n",
    "    \"\"\"\n",
    "    model_scores_map = {}\n",
    "    for img_id, val in pred_boxes.items():\n",
    "        for score in val['scores']:\n",
    "            if score not in model_scores_map.keys():\n",
    "                model_scores_map[score] = [img_id]\n",
    "            else:\n",
    "                model_scores_map[score].append(img_id)\n",
    "    return model_scores_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=0.5):\n",
    "    \"\"\"Calculates average precision at given IoU threshold.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (list of list of floats): list of locations of predicted\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: avg precision as well as summary info about the PR curve\n",
    "        Keys:\n",
    "            'avg_prec' (float): average precision for this IoU threshold\n",
    "            'precisions' (list of floats): precision value for the given\n",
    "                model_threshold\n",
    "            'recall' (list of floats): recall value for given\n",
    "                model_threshold\n",
    "            'models_thrs' (list of floats): model threshold value that\n",
    "                precision and recall were computed for.\n",
    "    \"\"\"\n",
    "    model_scores_map = get_model_scores_map(pred_boxes)\n",
    "    sorted_model_scores = sorted(model_scores_map.keys())\n",
    "    \n",
    "    # Sort the predicted boxes in descending order (lowest scoring boxes first).\n",
    "    for img_id in pred_boxes.keys():\n",
    "        arg_sort = np.argsort(pred_boxes[img_id]['scores'])\n",
    "        pred_boxes[img_id]['scores'] = np.array(pred_boxes[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_boxes[img_id]['boxes'] = np.array(pred_boxes[img_id]['boxes'])[arg_sort].tolist()\n",
    "    \n",
    "    pred_boxes_pruned = deepcopy(pred_boxes)\n",
    "    \n",
    "    precisions, recalls, model_threshs, img_results = [], [], [], {}\n",
    "    \n",
    "    # Loop over model score thresholds and calc precision, recall.\n",
    "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores_map[model_score_thr]\n",
    "        for img_id in img_ids:\n",
    "            gt_boxes_img = gt_boxes[img_id]\n",
    "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "            start_idx = 0\n",
    "            for score in box_scores:\n",
    "                if score <= model_score_thr:\n",
    "                    pred_boxes_pruned[img_id]\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break \n",
    "                    \n",
    "            # Remove boxes, scores for lower than thresh scores.\n",
    "            pred_boxes_pruned[img_id]['scores'] = pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "            pred_boxes_pruned[img_id]['boxes'] = pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "            \n",
    "            # Recalculate img results for this img.\n",
    "            img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr)\n",
    "        prec, rec = calc_precision_recall(img_results) \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        model_threshs.append(model_score_thr)\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    prec_at_rec = []\n",
    "    for recall_lev in np.linespace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args = np.argwhere(recalls >= recall_lev).flatten()\n",
    "            prec = max(precisions[args])\n",
    "        except ValueError:\n",
    "            prec = 0.0\n",
    "        prec_at_rec.append(prec)\n",
    "    avg_prec = np.mean(prec_at_rec)\n",
    "    \n",
    "    return {\n",
    "        \"avg_prec\": avg_prec,\n",
    "        \"precisions\": precisions,\n",
    "        \"recalls\": recalls,\n",
    "        \"model_thrs\": model_thrs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(\n",
    "    precisions, recalls, category='Person', label=None, color=None, ax=None):\n",
    "    \"\"\"Simple plotting helper function\"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if color is None:\n",
    "        color = COLORS[0]\n",
    "    ax.scatter(recalls, precisions, label=label, s=20, color=color)\n",
    "    ax.set_xlabel('recall')\n",
    "    ax.set_ylabel('precision')\n",
    "    ax.set_title('Precision-Recall curve for {}'.format(category))\n",
    "    ax.set_xlim([0.0,1.3])\n",
    "    ax.set_ylim([0.0,1.2])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
