{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the FPN Backbone for the RetinaNet Object Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "if find_spec(\"model\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import BaseModel\n",
    "from layers.upsample import LateralUpsampleMerge\n",
    "from layers.wrappers import conv3x3, conv1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retina FPN Backbone Variant\n",
    "\n",
    "paper: [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "Section 4. RetinaNet Detector -> Feature Pyramid Network Backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetFPN50(BaseModel):\n",
    "    \"\"\"\n",
    "    Implements FPN network assuming a ResNet50 backbone.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_features=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stage 7:\n",
    "        self.conv7_up = conv3x3(out_features, out_features, stride=2)\n",
    "        \n",
    "        # Stage 6:\n",
    "        self.conv6_up = conv3x3(2048, out_features, stride=2)\n",
    "        \n",
    "        # Stage 5:\n",
    "        self.lateral5 = conv1x1(2048, out_features)\n",
    "        self.conv5 = conv3x3(out_features, out_features)\n",
    "        \n",
    "        # Stage 4:\n",
    "        self.lat_merge4 = LateralUpsampleMerge(1024, out_features)\n",
    "        self.conv4 = conv3x3(out_features, out_features)\n",
    "        \n",
    "        # Stage 3:\n",
    "        self.lat_merge3 = LateralUpsampleMerge(512, out_features)\n",
    "        self.conv3  = conv3x3(out_features, out_features)\n",
    "        \n",
    "    def forward(self, C2, C3, C4, C5):\n",
    "        \n",
    "        # Stage 6 and 7 forward.\n",
    "        P6 = self.conv6_up(C5)\n",
    "        P7 = self.conv7_up(F.relu(P6))\n",
    "                           \n",
    "        # Stage 5 forward.\n",
    "        out = self.lateral5(C5)\n",
    "        P5 = self.conv5(out)  \n",
    "        \n",
    "        # Stage 4 forward.\n",
    "        out = self.lat_merge4(out, C4)\n",
    "        P4 = self.conv4(out)\n",
    "        \n",
    "        # Stage 3 forward.\n",
    "        out = self.lat_merge3(out, C3)\n",
    "        P3 = self.conv3(out)\n",
    "        \n",
    "        return P3, P4, P5, P6, P7\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetinaNetFPN50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinaNetFPN50(\n",
       "  (conv7_up): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv6_up): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lat_merge4): LateralUpsampleMerge(\n",
       "    (lat_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lat_merge3): LateralUpsampleMerge(\n",
       "    (lat_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
